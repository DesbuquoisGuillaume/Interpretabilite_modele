{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interprétabilité des modèles\n",
    "## 0. Overview\n",
    "\n",
    "La montée en complexité des modèles de Machine learning et Deep learning introduit bien souvent des modèles avec d'importantes quantités de paramètres, rendant leurs intérprétations complexes.\n",
    "\n",
    "Pourtant, il est très important d'être capable de comprendre les décisions prises par un modèle. Cela permet avant tout de les optimiser, mais aussi de les rendre plus accessibles pour des pôles plus orientés métiers.\n",
    "\n",
    "=> **L'objectif de ce notebook est la mise en pratique de l'interprétabilité des modèles sur des cas concrets**\n",
    "\n",
    "### 0.1 Données\n",
    "\n",
    "Pour la suite de ce notebook, nous nous baserons sur 2 jeux de données connus :\n",
    "* **boston** : ce dataset vise à déterminer le prix de vente des maisons de Boston en fonction de différents indicateurs\n",
    "* **iris** : ce dataset vise à classifier des iris suivant 3 catégories en fonction de différentes informations sur les fleurs\n",
    "\n",
    "Nous commencerons par utiliser le jeu de données **boston**. Son importation est disponible ci-dessous. Vous retrouverez aussi le dictionnaire des variables présentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target, columns=[\"Houses prices\"])\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Focus sur les Forêts Aléatoires en Régression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings # On enlève les warnings pour la suite du notebook\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treeinterpreter import treeinterpreter as ti, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Idée générale\n",
    "\n",
    "Beaucoup d'ouvrages traitant des **Forêts Aléatoires** considèrent ces modèles comme *black-blox*. En cause : le grand nombre d'arbres profonds, où chaque arbre est entrainé sur un échantillon bootstrap, et ou chaque noeud de l'arbre est partitionné sur la base d'un sous échantillon de variables tiré aléatoirement.\n",
    "\n",
    "Il est donc compliqué d'avoir une compréhension complète du processus de décision. Une solution généralement apportée est de calculer l'importance des variables du modèle. L'idée est de permuter les valeurs de chaque variable une à une, et de déterminer quelle permutation à le plus d'impact sur la performance du modèle.\n",
    "\n",
    "\n",
    "L'approche sur laquelle nous allons nous baser est différente, et s'appuie sur les **chemins de décisions** formés par les arbres. Lorsque l'on s'intéresse à un arbre de décision, il est assez intuitif qu'il existe, pour chaque décision, un chemin partant du noeud racine jusqu'à un noeud terminal. Ce chemin est constitué d'une série de décisions basées sur une ou plusieurs variables, qui contribuent donc à la prédiction.\n",
    "\n",
    "#### 1.2.1 L'approche des chemins de décision\n",
    "\n",
    "Un arbre de décision avec $M$ noeuds divise l'espace des données en $M$ régions, notées $R_m, 1≤m≤M$. De manière générale, si l'on définit $x$ une nouvelle observation et $f$ la fonction de prédiction de l'arbre, on a :\n",
    "\n",
    "<center> $ f(x) = \\sum\\limits_{m=1}^{M}c_m \\mathbb{1}_{(x\\ \\in \\ R_m)} $ </center> \n",
    "\n",
    "Avec :\n",
    "* $c_m$ la valeur de sortie pour la région $R_m$. $c_m$ est déterminée lors de la phase d'apprentissage de l'arbre\n",
    "* $\\mathbb{1}_{(x\\ \\in \\ R_m)}$ la fonction indicatrice renvoyant 1 si $x\\ \\in \\ R_m$, 0 sinon\n",
    "\n",
    "##### 1.2.1.1 L'exemple d'un arbre de régression\n",
    "\n",
    "Partons maintenant d'un exemple pour comprendre l'approche des chemins de décision. Nous nous baserons sur le jeu de données **boston**. Pour rappel, on cherche à déterminer le prix des maisons de Boston en fonction de différents indicateurs.\n",
    "\n",
    "\n",
    "<img src=\"Tree2.png\" alt=\"Drawing\" style=\"width: 400px;\" align=\"left\"/>\n",
    "L'image à gauche montre un arbre de régression de pronfondeur 3. On retrouve un champ *Value* en dessous de chaque noeud. Ce dernier représente la moyenne des prix des maisons, et est logiquement différent suivant l'emplacement ou l'on se situe dans l'arbre.\n",
    "\n",
    "Cela permet de suivre l'évolution des prix des maisons suivant les coupures réalisées par différentes variables.\n",
    "\n",
    "Le but principal de cet exemple est de faire apparaître qu'il existe un moyen \"opérationnel\" de décomposer les prédictions au travers des **chemins de décision**.\n",
    "\n",
    "La *Value* du noeud racine (i.e. *22.60* ici) correspond à la moyenne des prix des maisons sur les données d'apprentissage, nous l'appellerons **priormean** par la suite. Ainsi, chaque prédiction partant du noeud racine, peut être définie comme le **priormean** auquel on soustrait ou ajoute les contributions des variables sur le chemin de décision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par exemple, pour une nouvelle observation $x$ dont la variable $RM$ est inférieure à *6.94*, on se retrouve à descendre la première branche verte de l'arbre. A ce niveau, les observations de l'échantillon d'apprentissage ont une moyenne des prix des maisons de *37.42*. Le priormean étant de *22.60*, on peut considérer que la contribution de la variable $RM$ à été de faire augmenter la prédiction de *14.82* (*37.42-22.60*).\n",
    "\n",
    "L'opération est donc à répeter au fur et à mesure de la descente de l'arbre jusqu'au noeud terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de prédiction peut donc se réécrire :\n",
    "\n",
    "<center> $ f(x) = c_{full} + \\sum\\limits_{k=1}^{K}contrib(x,k) $ </center> \n",
    "\n",
    "Avec :\n",
    "* $K$ le nombre de variables\n",
    "* $cfull$ la valeur du noeud racine (*Value* sur l'arbre ci-dessus)\n",
    "* $contrib(x, k)$ la contribution de la $k$-ième variable à la prédiction de $x$. \n",
    "\n",
    "\n",
    "On retrouve quelque chose qui, en apparence, ressemble à une régression linéaire du type $f(x) = ax + b$.\n",
    "Toutefois, en régression linéaire, le vecteur de paramètres $a$ est constant. Dans le cas de l'arbre de décision, les contributions des variables varient suivant le chemin de décision impliqué.\n",
    "\n",
    "Cela permet cependant de décomposer une prédiction et donc de pouvoir plus facilement l'interpréter.\n",
    "\n",
    "#### 1.2.1.2 Le passage aux Forêts Aléatoires\n",
    "\n",
    "L'application de ce principe d'un arbre de décision à une Forêt Aléatoire est relativement simple. En effet, la prédiction d'une forêt est la moyenne des prédictions de ses arbres. Soit $g$ la fonction de prédiction d'une forêt, et $x$ une nouvelle observation. On a :\n",
    "\n",
    "<center> $ g(x) = \\frac{1}{J} \\sum\\limits_{j=1}^{J}f_j(x) $ </center> \n",
    "\n",
    "Avec :\n",
    "* $J$ le nombre d'arbres de la forêt\n",
    "* $f_j(x)$ la préction pour la nouvelle observation $x$ pour l'arbre $j$\n",
    "\n",
    "\n",
    "On peut donc facilement écrire :\n",
    "\n",
    "<center> $ g(x) = \\frac{1}{J} \\sum\\limits_{j=1}^{J}c_{j_{full}} + \\sum\\limits_{k=1}^{K}(\\frac{1}{J}\\sum\\limits_{j=1}^{J}contrib_j(x,k)) $ </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Première décomposition - Simplification du problème\n",
    "\n",
    "Nous allons maintenant chercher à appliquer la décomposition vue précédemment.\n",
    "\n",
    "Dans un premier temps, nous essaierons de décomposer le prix estimé d'une maison $i$ (notre variable cible) comme la somme des contributions de chaque variable pour cette maison, i.e. : \n",
    "<center> $ prediction^i=priormean+contributionVariable_1^i+…+contributionVariable_n^i $ </center> \n",
    "\n",
    "Peu de packages proposent actuellement de rentrer dans ce niveau de détails à l'heure actuelle. Nous nous baserons sur **treeinterpreter**. Ce dernier propose la décomposition exposée au dessus pour différents modèles existants sous *scikit-learn*, tels que :\n",
    "* DecisionTreeRegressor\n",
    "* DecisionTreeClassifier\n",
    "* ExtraTreeRegressor\n",
    "* ExtraTreeClassifier\n",
    "* RandomForestRegressor\n",
    "* RandomForestClassifier\n",
    "* ExtraTreesRegressor\n",
    "* ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Première Forêt Aléatoire\n",
    "**Exercice 1 :**\n",
    "Pour commencer, séparez les données à disposition en 2 échantillons (via la fonction *train_test_split*) en définissant une graîne aléatoire (=1234) :\n",
    "* un échantillon d'apprentissage (2/3 des données)\n",
    "* un échantillon de validation (1/3 des données)\n",
    "\n",
    "Entraînez ensuite une forêt (les paramètres par défaut suffiront pour l'exemple) sur la base de vos données d'apprentissage, en indiquant une graine aléatoire (=1234) pour figer les résultats.\n",
    "\n",
    "*NB : Nous noterons X_train, X_test, y_train, y_test nos échantillons d'apprentissage et de validation, et rf le modèle entraîné*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=1234, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=1/3, random_state=1234)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=1234)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant choisir 2 points de données de notre échantillon de test, sur lequel nous allons prédire notre cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction pour ligne 0 de notre échantillon de test : 32.18\n",
      "Prédiction pour ligne 1 de notre échantillon de test : 23.78\n"
     ]
    }
   ],
   "source": [
    "tworows = X_test.iloc[:2,]\n",
    "for i,prediction in enumerate(rf.predict(tworows)):\n",
    "    print(\"Prédiction pour ligne {} de notre échantillon de test : {}\".format(i,round(prediction,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que les prédictions sont très éloignées pour ces 2 points de données. L'idée est donc de comprendre maintenant quelles sont les variables qui ont le plus contribuées (aussi bien négativement que positivement) aux prédictions.\n",
    "\n",
    "Pour cela, nous allons réaliser la décomposition vue précédemment en utilisant le package **treeinterpreter**.\n",
    "\n",
    "La structure est relativement simple, et nous permet, sur la base d'un modèle déjà entraîné, de récupérer pour des points de données de l'échantillon de test, la prédiction du modèle, le priormean, ainsi que les contributions de chaque variable :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, priormean, contributions = ti.predict(rf, tworows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'on commence par s'intéresser au contenu de *prediction*, on remarque bien que l'on récupère les mêmes valeurs que ci-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32.18],\n",
       "       [23.78]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le contenu de la variable *priormean*, qui pour rappel correspond à la moyenne de notre variable cible sur l'échantillon d'apprentissage, est logiquement le même qu'importe le point de données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.424273, 22.424273])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priormean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, la variable *contributions* contient deux arrays de dimensions *1x13* chacun, représentant pour chaque prédiction, la contribution de chacune des variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.60555556e-01,  0.00000000e+00,  7.50000000e-02,\n",
       "         0.00000000e+00,  2.46911765e-01,  6.68611695e+00,\n",
       "         4.71296296e-03, -9.32726952e-01,  4.50000000e-02,\n",
       "         1.76981430e+00,  1.61194290e-01, -2.28973138e-01,\n",
       "         1.26812127e+00],\n",
       "       [-2.89914054e-01, -8.12566845e-02, -7.07142857e-02,\n",
       "         0.00000000e+00,  1.82911765e-01,  1.84975041e-01,\n",
       "        -2.87870370e-02,  7.94091021e-02,  2.83180189e-01,\n",
       "        -3.20409387e-02, -1.88606092e+00, -6.34404762e-02,\n",
       "         3.07746530e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc tout ce qui est nécessaire pour déterminer les contributions de chacune des variables aux deux prédictions, en rappelant que :\n",
    "<center> $ prediction=priormean+contributionVariable_1+…+contributionVariable_n $ </center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point de données 0\n",
      "Prior mean 22.424272997032645\n",
      "Contributions des variables (par décroissance absolue) :\n",
      "RM : 6.69\n",
      "TAX : 1.77\n",
      "LSTAT : 1.27\n",
      "DIS : -0.93\n",
      "CRIM : 0.66\n",
      "NOX : 0.25\n",
      "B : -0.23\n",
      "PTRATIO : 0.16\n",
      "INDUS : 0.07\n",
      "RAD : 0.05\n",
      "AGE : 0.0\n",
      "ZN : 0.0\n",
      "CHAS : 0.0\n",
      "--------------------\n",
      "Point de données 1\n",
      "Prior mean 22.424272997032645\n",
      "Contributions des variables (par décroissance absolue) :\n",
      "LSTAT : 3.08\n",
      "PTRATIO : -1.89\n",
      "CRIM : -0.29\n",
      "RAD : 0.28\n",
      "RM : 0.18\n",
      "NOX : 0.18\n",
      "ZN : -0.08\n",
      "DIS : 0.08\n",
      "INDUS : -0.07\n",
      "B : -0.06\n",
      "TAX : -0.03\n",
      "AGE : -0.03\n",
      "CHAS : 0.0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tworows)):\n",
    "    print(\"Point de données {}\".format(i))\n",
    "    print(\"Prior mean {}\".format(priormean[i]))\n",
    "    print(\"Contributions des variables (par décroissance absolue) :\")\n",
    "    for c, feature in sorted(zip(contributions[i], \n",
    "                                 boston.feature_names), \n",
    "                             key=lambda x: -abs(x[0])):\n",
    "        print(\"{} : {}\".format(feature, round(c, 2)))\n",
    "    print(\"-\"*20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2** : A partir des informations récupérées (*priormean* et *contributions*) et sur la base de la décomposition exposée, recalculer les prédictions pour nos 2 points de données de test. Vérifiez qu'elles correspondent bien à ce que nous avions obtenu avec *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[32.18],\n",
       "        [23.78]]), array([32.18, 23.78]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction,priormean + np.sum(contributions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce package est donc très pratique pour pouvoir mieux interpréter les prédictions de nos forêts aléatoires pour certains points de données.\n",
    "\n",
    "Le package possède **2 intérêts majeurs** :\n",
    "* Comprendre pourquoi les valeurs prédites sur 2 jeux de données sont différentes, et quelles sont les variables en causes. Sur le jeu de données *boston*, on pourrait par exemple chercher à comprendre d'où viennent les différences de prix des maisons de plusieurs voisinages\n",
    "* Débugger un modèle et/ou les données, en cherchant par exemple à comprendre pourquoi les valeurs prédites sur un nouveau jeu de données ne matchent pas avec celles d'anciennes données\n",
    "\n",
    "=> **Essayons de développer le premier cas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3** : Splitter le jeu de données de test *(X_test)* en 2 sous échantillons (respectivement *ech1* & *ech2*) de tailles égales. Ces échantillons modéliseront 2 voisinages. En utilisant la forêt déjà entraînée, calculez et stockez les prédictions associées à ces 2 sous échantillons. Calculez ensuite la moyenne des prédictions pour *ech1* et *ech2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.977142857142855 21.662823529411764\n"
     ]
    }
   ],
   "source": [
    "idx = round(len(X_test)/2)\n",
    "ech1 = X_test.iloc[:idx,:]\n",
    "ech2 = X_test.iloc[idx:,:]\n",
    "\n",
    "pred_ech1 = rf.predict(ech1)\n",
    "pred_ech2 = rf.predict(ech2)\n",
    "print(np.mean(pred_ech1),np.mean(pred_ech2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut constater que les prédictions moyennes sont relativement différentes sur les 2 échantillons.\n",
    "\n",
    "**Exerice 4** : Appliquer la décomposition vue précedemment sur les 2 sous échantillons *ech1* et *ech2*. On notera *prediction1, priormean1* et *contributions1* (respectivement *2*) les variables dans lequelles seront stockées les résultats.\n",
    "\n",
    "Moyennez ensuite les contributions par variable pour chaque sous échantillon et stockez les résultats dans deux variables, respectivement *totalc1* et *totalc2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1, priormean1, contributions1 = ti.predict(rf, ech1)\n",
    "prediction2, priormean2, contributions2 = ti.predict(rf, ech2)\n",
    "\n",
    "totalc1 = np.mean(contributions1, axis=0) \n",
    "totalc2 = np.mean(contributions2, axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la mesure où les *priormean* sont les mêmes (puisque calculés sur le même échantillon d'apprentissage), la différence entre les prédictions moyennes sur les 2 sous échantillons provient uniquement des contributions des différentes variables.\n",
    "En outre, la différence entre les contributions des variables est égale à la différence entre les prédictions. Ce que nous pouvons facilement vérifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3143193277310923, 1.3143193277310914)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(totalc1 - totalc2),np.mean(prediction1) - np.mean(prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5** : Calculer les différences de contributions de chaque variable entre les 2 sous échantillons, et appuyez vous sur le dictionnaire des données disponible en début de notebook pour interpréter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTAT : 0.45\n",
      "RM : 0.43\n",
      "DIS : 0.19\n",
      "B : 0.13\n",
      "AGE : 0.11\n",
      "PTRATIO : 0.08\n",
      "RAD : 0.05\n",
      "TAX : 0.03\n",
      "ZN : 0.03\n",
      "CHAS : 0.0\n",
      "INDUS : -0.02\n",
      "CRIM : -0.07\n",
      "NOX : -0.1\n"
     ]
    }
   ],
   "source": [
    "for c, variable in sorted(zip(totalc1 - totalc2, \n",
    "                             boston.feature_names), reverse=True):\n",
    "    print('{} : {}'.format(variable, round(c, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Seconde décomposition\n",
    "\n",
    "Rendre les prévisions de nos forêts aléatoires intérprétable semble donc assez simple, et relativement proche de ce que l'on connaît avec des modèles linéaires.\n",
    "\n",
    "Cependant, **cette décomposition est imparfaite, puisqu'elle ne prend pas en compte les intéractions entre variables.**\n",
    "Pour l'illustrer, prenons l'exemple du XOR (= ou exclusif).\n",
    "\n",
    "##### 1.2.4.1 L'exemple du XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Sortie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1  X2  Sortie\n",
       "0   0   0       0\n",
       "1   0   1       1\n",
       "2   1   0       1\n",
       "3   1   1       0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'X1': [0, 0, 1, 1], 'X2': [0, 1, 0, 1], 'Sortie': [0, 1, 1, 0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **XOR** est relativement facile à comprendre. Pour que la valeur de sortie soit Vraie (=1), il faut que l'une seule des valeurs de *X1* ou *X2* soit Vraie (=1).\n",
    "\n",
    "Dans ce cas, ni *X1* ni *X2* n'apporte de l'information sur la valeur de sortie seul. C'est à dire que si l'on isole *X1* ou *X2*, il n'est pas possible de prédire la valeur de *Sortie*.\n",
    "\n",
    "Un arbre de décision va être capable d'apprendre de cet effet et donc de classifier correctement le XOR (arbre ci dessous).\n",
    "\n",
    "<img src=\"Tree.png\" alt=\"Drawing\" style=\"width: 400px;\" align=\"left\"/>\n",
    "\n",
    "Si l'on se penche sur le premier noeud de l'arbre, où seul *X1* est connu, nous ne sommes pas en mesure de savoir si la valeur de *Sortie* est 1 ou 0. La meilleure prédiction possible est donc 0.5 (ce qui revient à dire \"Je ne sais pas\"). De ce fait, la contribution de *X1* si l'on ne considère que ce noeud serait (à tord) 0. *(Un priormean qui vaut 0.50, et les *Value* des deux noeuds suivants aussi à 0.50)*\n",
    "\n",
    "Aux noeuds suivants de l'arbre, nous prenons connaissance de la valeur de *X2*. Nous pouvons donc aisément réaliser la bonne prédiction.\n",
    "Toutefois, attribuer la bonne prédiction à la variable *X2* seule serait faux, puisque **c'est l'intéraction des deux variables qui nous permet de bien prédire la valeur de *sortie*.**\n",
    "Il faut donc répartir la contribution équitablement entre les deux variables.\n",
    "\n",
    "Prenons le <font color='red'>chemin rouge</font> à titre d'exemple. Ce chemin conduit à la prédiction de la valeur 0.\n",
    "\n",
    "On sait que le priormean du modèle est de 0.50.\n",
    "\n",
    "Pour prédire la valeur 0.00, sachant que le *priormean* est de 0.50, et que la contribution de *X1* (i.e. premier noeud) est de 0.00. La contribution de l'intéraction *X1X2* est forcément de -0.50.\n",
    "\n",
    "Cf la décomposition :\n",
    "<center> $ 0.00 = 0.50 (priormean) + 0.00(contrib X1) - 0.5 (contrib X1X2) $ </center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les **effets d'intéractions** peuvent être calculés par le package **treeinterpreter** en passant le paramètre *joint_contribution=True* à la fonction *.predict*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 6**:  Repartir des 2 sous échantillons *ech1* et *ech2* construit à l'**Exercice 3**. Appliquer de nouveau la décomposition vue à l'exercice 4 en faisant attention à inclure les effets d'intéractions. Stockez à nouveau les résultats dans les variables *prediction1, priormean1, contributions1* (respectivement *2* pour l'*ech2*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1, priormean1, contributions1 = ti.predict(rf, ech1, joint_contribution=True)\n",
    "prediction2, priormean2, contributions2 = ti.predict(rf, ech2, joint_contribution=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction *aggregated_contribution* du module *utils* (*utils.aggregated_contribution*), avec en paramètre les contributions obtenues par la fonction *.predict* est très utile lorsque l'on utilise des effets d'intéractions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(5, 7, 9, 11, 12): array([0.01897795]),\n",
       " (4, 5, 12): array([-0.01025543]),\n",
       " (5, 9, 11, 12): array([0.00703231]),\n",
       " (5, 7, 9): array([0.00735771]),\n",
       " (0, 5, 7, 9, 10, 12): array([0.0085119]),\n",
       " (5, 9, 10, 12): array([0.0064087]),\n",
       " (12,): array([0.0442942]),\n",
       " (5, 9, 12): array([0.01418146]),\n",
       " (5, 6, 10, 12): array([-0.01833973]),\n",
       " (5, 6, 7, 10, 12): array([0.00697784]),\n",
       " (0, 5, 6, 7, 9, 10, 12): array([0.00349838]),\n",
       " (4, 5, 7, 10, 12): array([0.00652315]),\n",
       " (2, 5, 7, 9, 12): array([0.00524762]),\n",
       " (5, 7, 8, 9, 11): array([0.00084184]),\n",
       " (0, 5, 9, 10, 12): array([-0.0096962]),\n",
       " (0, 5, 10, 11, 12): array([0.01533069]),\n",
       " (5, 12): array([0.22056535]),\n",
       " (5, 7): array([0.03366707]),\n",
       " (4, 5, 7, 12): array([0.00468152]),\n",
       " (5, 7, 10, 12): array([0.03994283]),\n",
       " (5, 7, 9, 11): array([-0.00125]),\n",
       " (5,): array([0.23839139]),\n",
       " (5, 10, 12): array([0.09733232]),\n",
       " (5, 7, 12): array([0.01339164]),\n",
       " (5, 7, 9, 10, 12): array([0.00825132]),\n",
       " (5, 10, 11, 12): array([0.03105599]),\n",
       " (5, 7, 9, 12): array([-0.014303]),\n",
       " (5, 7, 8, 12): array([0.02267758]),\n",
       " (0, 5, 6, 7, 8, 9, 11, 12): array([-0.00206448]),\n",
       " (0, 1, 5, 7, 10, 12): array([-0.0025359]),\n",
       " (0, 1, 5, 12): array([0.01006035]),\n",
       " (0, 1, 5, 10, 12): array([-0.00293493]),\n",
       " (0, 4, 5, 6, 7, 8, 9, 11, 12): array([-0.00133333]),\n",
       " (0, 5, 7, 10, 11, 12): array([-0.00291667]),\n",
       " (0, 5, 12): array([-0.07767455]),\n",
       " (0, 5, 7, 8, 9, 11, 12): array([-0.00421205]),\n",
       " (0, 5, 6, 7, 10, 12): array([0.00306638]),\n",
       " (5, 7, 8, 9, 10, 12): array([0.0039881]),\n",
       " (2, 5, 7, 8, 12): array([-0.00265306]),\n",
       " (0, 5, 10, 12): array([0.00047238]),\n",
       " (0, 5, 7, 10, 12): array([-0.00041667]),\n",
       " (5, 7, 8, 9, 11, 12): array([-0.00508671]),\n",
       " (7, 12): array([-0.02472102]),\n",
       " (0, 7, 9, 12): array([-0.00566757]),\n",
       " (0, 7, 12): array([0.00965064]),\n",
       " (0, 4, 11, 12): array([0.00197846]),\n",
       " (0, 6, 7, 9, 11, 12): array([0.00021825]),\n",
       " (0, 4, 7, 11, 12): array([-0.00618311]),\n",
       " (0, 4, 5, 7, 11, 12): array([5.12156455e-18]),\n",
       " (7, 9, 12): array([0.00788585]),\n",
       " (0, 5, 7, 9, 12): array([-0.00356641]),\n",
       " (0, 12): array([-0.08148901]),\n",
       " (0, 4, 7, 12): array([-0.05539849]),\n",
       " (0, 4, 12): array([0.00593521]),\n",
       " (4, 7, 12): array([0.00079586]),\n",
       " (0, 4, 6, 11, 12): array([-0.00126134]),\n",
       " (0, 4, 5, 7, 12): array([-0.00834609]),\n",
       " (0, 4, 6, 7, 11, 12): array([0.00137755]),\n",
       " (4, 12): array([-0.00039709]),\n",
       " (0, 7, 9, 11, 12): array([-0.00054989]),\n",
       " (0, 5, 7, 9, 11, 12): array([-0.00424651]),\n",
       " (0, 5, 6, 7, 9, 11, 12): array([-0.00396825]),\n",
       " (0, 4, 5, 12): array([-0.0252928]),\n",
       " (4, 7, 11, 12): array([0.01461224]),\n",
       " (2, 5, 6, 7, 9, 12): array([0.00128968]),\n",
       " (0, 5, 6, 12): array([-0.00919989]),\n",
       " (5, 6, 7, 12): array([0.00071665]),\n",
       " (5, 8, 10, 11, 12): array([-0.00618354]),\n",
       " (0, 5, 7, 12): array([0.01204609]),\n",
       " (0, 2, 5, 6, 7, 12): array([-5.58608059e-05]),\n",
       " (0, 2, 5, 7, 8, 9, 11, 12): array([-0.01006181]),\n",
       " (2, 5, 6, 7, 12): array([0.00689243]),\n",
       " (5, 6, 8, 10, 11, 12): array([-0.00290122]),\n",
       " (4, 5, 6, 8, 10, 11, 12): array([2.38095238e-05]),\n",
       " (5, 7, 10, 11, 12): array([0.00342211]),\n",
       " (4, 7, 10, 12): array([-0.0061553]),\n",
       " (6, 7, 12): array([-0.02236156]),\n",
       " (0, 4, 5, 6, 12): array([-0.0067948]),\n",
       " (4, 6, 7, 9, 10, 12): array([-0.00166103]),\n",
       " (5, 7, 8, 10, 12): array([0.00134648]),\n",
       " (0, 4, 6, 7, 9, 10, 12): array([0.00066667]),\n",
       " (4, 6, 7, 10, 12): array([-0.00250396]),\n",
       " (5, 6, 7, 9, 12): array([0.01197596]),\n",
       " (3, 6, 7, 11, 12): array([-0.00274934]),\n",
       " (3, 6, 7, 12): array([0.00342536]),\n",
       " (5, 7, 11, 12): array([-0.00498547]),\n",
       " (0, 6, 7, 11, 12): array([0.00267857]),\n",
       " (0, 3, 6, 7, 11, 12): array([0.00071429]),\n",
       " (5, 6, 7, 11, 12): array([-0.0010189]),\n",
       " (6, 7, 11, 12): array([-0.00204115]),\n",
       " (0, 7, 11, 12): array([0.0050439]),\n",
       " (0, 6, 7, 12): array([0.00190476]),\n",
       " (0, 5, 7, 11, 12): array([0.00715675]),\n",
       " (1, 5, 7, 9, 12): array([-0.00071429]),\n",
       " (0, 1, 5, 7, 8, 12): array([-0.00054563]),\n",
       " (0, 2, 5, 11, 12): array([0.00158333]),\n",
       " (0, 5, 7, 8, 9, 10, 12): array([-0.00068027]),\n",
       " (5, 7, 8, 9, 12): array([-0.00777486]),\n",
       " (5, 8, 9, 10, 12): array([-0.00579561]),\n",
       " (1, 5, 7, 8, 10, 12): array([0.00476791]),\n",
       " (2, 5, 7, 8, 9, 12): array([-0.00452513]),\n",
       " (1, 2, 5, 7, 8, 9, 12): array([0.00303571]),\n",
       " (2, 5, 7, 12): array([-0.00159456]),\n",
       " (0, 1, 5, 7, 12): array([-0.01477976]),\n",
       " (2, 5, 12): array([0.00862616]),\n",
       " (1, 2, 5, 6, 7, 8, 9, 12): array([0.00196429]),\n",
       " (0, 1, 5, 6, 7, 8, 12): array([-0.00222222]),\n",
       " (4, 5, 7, 9, 11, 12): array([0.00316114]),\n",
       " (2, 5, 11, 12): array([-0.02117657]),\n",
       " (4, 5, 6, 7, 9, 11, 12): array([-0.00772066]),\n",
       " (2, 4, 5, 7, 12): array([-0.00195238]),\n",
       " (5, 7, 9, 10, 11, 12): array([0.00472534]),\n",
       " (1, 2, 5, 7, 8, 10, 12): array([-0.00112271]),\n",
       " (0, 2, 5, 10, 12): array([-0.00343458]),\n",
       " (0, 5, 8, 9, 10, 12): array([-0.0062224]),\n",
       " (0, 2, 5, 6, 11, 12): array([-0.00033333]),\n",
       " (0, 2, 5, 6, 9, 12): array([0.00095919]),\n",
       " (0, 4, 6, 7, 10, 11, 12): array([-0.00047619]),\n",
       " (2, 4, 5, 10, 12): array([-0.01095373]),\n",
       " (0, 2, 4, 7, 11, 12): array([-0.00031746]),\n",
       " (2, 4, 5, 7, 10, 11, 12): array([0.00080357]),\n",
       " (4, 5, 10, 12): array([0.0046421]),\n",
       " (0, 5, 6, 9, 11, 12): array([-0.00071098]),\n",
       " (2, 4, 5, 6, 11, 12): array([0.00653912]),\n",
       " (4, 5, 6, 7, 11, 12): array([0.00398238]),\n",
       " (0, 4, 5, 11, 12): array([0.00442063]),\n",
       " (0, 5, 9, 12): array([0.02257561]),\n",
       " (4, 5, 7, 10, 11, 12): array([-0.00182197]),\n",
       " (6, 7, 10, 11, 12): array([-0.00030824]),\n",
       " (2, 4, 5, 6, 10, 11, 12): array([-0.00257937]),\n",
       " (2, 4, 5, 9, 10, 12): array([-0.00155811]),\n",
       " (0, 5, 6, 9, 12): array([-0.00226903]),\n",
       " (2, 4, 5, 12): array([-0.01738923]),\n",
       " (0, 2, 5, 6, 12): array([-0.00874145]),\n",
       " (7, 11, 12): array([-0.00386045]),\n",
       " (4, 6, 7, 9, 10, 11, 12): array([0.00030423]),\n",
       " (0, 6, 7, 10, 11, 12): array([-0.0023648]),\n",
       " (0, 2, 5, 6, 9, 11, 12): array([0.00012843]),\n",
       " (2, 4, 5, 6, 12): array([-0.01231122]),\n",
       " (0, 2, 4, 5, 9, 10, 12): array([0.00153274]),\n",
       " (4, 6, 7, 10, 11, 12): array([-0.00169511]),\n",
       " (11, 12): array([0.00650128]),\n",
       " (4, 5, 6, 7, 10, 11, 12): array([0.00267857]),\n",
       " (0, 5, 6, 7, 12): array([-0.00045133]),\n",
       " (3, 5, 6, 7, 11, 12): array([-0.00166667]),\n",
       " (0, 7, 10, 11, 12): array([-0.00013889]),\n",
       " (0, 4, 5, 7, 10, 11, 12): array([-0.00122024]),\n",
       " (4, 5, 6, 7, 12): array([0.00562453]),\n",
       " (4, 7, 9, 10, 12): array([0.00282107]),\n",
       " (0, 5, 7, 8, 9, 10, 11, 12): array([-0.00129252]),\n",
       " (1, 7, 9, 10, 11, 12): array([-0.0015873]),\n",
       " (0, 5, 6, 7, 8, 9, 10, 11, 12): array([0.00045918]),\n",
       " (2, 4, 5, 7, 10, 12): array([-0.00089286]),\n",
       " (5, 6, 7, 8, 10, 12): array([-0.00119738]),\n",
       " (1, 4, 5, 6, 7, 12): array([0.00132653]),\n",
       " (6, 7, 10, 12): array([0.00768707]),\n",
       " (4, 7, 9, 12): array([-0.00262662]),\n",
       " (7, 9, 10, 12): array([0.00434921]),\n",
       " (7, 10, 12): array([-0.00242517]),\n",
       " (1, 7, 9, 10, 12): array([-0.00059524]),\n",
       " (2, 5, 10, 12): array([0.00126136]),\n",
       " (2, 5, 6, 8, 10, 11, 12): array([-0.00014881]),\n",
       " (5, 6, 7, 9, 10, 12): array([0.00631743]),\n",
       " (2, 5, 6, 10, 12): array([0.00087302]),\n",
       " (1, 5, 9, 10, 12): array([-0.00375]),\n",
       " (2, 5, 6, 7, 9, 10, 12): array([0.00416883]),\n",
       " (2, 5, 6, 10, 11, 12): array([-0.00018849]),\n",
       " (0, 1, 5, 6, 7, 12): array([0.00095238]),\n",
       " (0, 4, 5, 7, 9, 12): array([0.00134127]),\n",
       " (2, 5, 6, 8, 9, 10, 11, 12): array([-5.95238095e-05]),\n",
       " (5, 6, 7, 10, 11, 12): array([-0.00122732]),\n",
       " (0, 3, 5, 7, 9, 12): array([-0.00457143]),\n",
       " (0, 3, 5, 9, 12): array([-0.00179221]),\n",
       " (4, 7, 10, 11, 12): array([-0.00113095]),\n",
       " (0, 5, 6, 7, 10, 11, 12): array([-0.00059127]),\n",
       " (5, 6, 7, 8, 11, 12): array([0.00160053]),\n",
       " (4, 11, 12): array([0.0100202]),\n",
       " (4, 5, 8, 9, 10, 12): array([0.00035714]),\n",
       " (0, 2, 5, 10, 11, 12): array([0.00096032]),\n",
       " (0, 2, 5, 7, 10, 11, 12): array([4.29550575e-18]),\n",
       " (0, 5, 6, 7, 8, 10, 12): array([-0.0004828]),\n",
       " (0, 1, 5, 6, 7, 10, 12): array([0.00011905]),\n",
       " (4, 6, 7, 8, 9, 10, 12): array([-0.00019841]),\n",
       " (0, 4, 5, 6, 7, 8, 10, 12): array([0.00017857]),\n",
       " (4, 5, 6, 7, 9, 10, 12): array([-0.00685999]),\n",
       " (2, 4, 5, 6, 7, 9, 11, 12): array([9.52380952e-05]),\n",
       " (5, 6, 7, 9, 11, 12): array([0.00050397]),\n",
       " (4, 5, 6, 7, 8, 10, 12): array([-0.00198413]),\n",
       " (0, 2, 5, 6, 7, 9, 11, 12): array([-1.98412698e-05]),\n",
       " (0, 4, 5, 6, 7, 8, 10, 11, 12): array([-3.96825397e-05]),\n",
       " (4, 5, 7, 9, 10, 12): array([-8.73015873e-05]),\n",
       " (0, 4, 5, 6, 10, 12): array([-0.0045432]),\n",
       " (2, 4, 5, 6, 8, 9, 10, 11, 12): array([-3.96825397e-05]),\n",
       " (4, 5, 6, 7, 8, 10, 11, 12): array([0.00044643]),\n",
       " (2, 4, 5, 8, 9, 10, 11, 12): array([-0.00175519]),\n",
       " (0, 4, 5, 6, 7, 9, 10, 12): array([-0.00029762]),\n",
       " (2, 4, 5, 8, 9, 10, 12): array([-0.00071023]),\n",
       " (2, 5, 6, 7, 9, 11, 12): array([-0.00012698]),\n",
       " (4, 5, 6, 7, 8, 9, 10, 12): array([0.00012698]),\n",
       " (0, 2, 4, 5, 6, 12): array([0.00215873]),\n",
       " (3, 4, 5, 12): array([0.00614286]),\n",
       " (5, 6, 8, 9, 10, 11, 12): array([-3.96825397e-05]),\n",
       " (3, 4, 5, 8, 12): array([0.001]),\n",
       " (3, 4, 12): array([-0.00316027]),\n",
       " (0, 4, 5, 8, 9, 10, 12): array([0.00047619]),\n",
       " (2, 5, 6, 7, 10, 11, 12): array([0.00039683]),\n",
       " (0, 6, 7, 10, 12): array([0.00062794]),\n",
       " (6, 7, 8, 10, 12): array([-0.00031548]),\n",
       " (4, 6, 7, 8, 10, 12): array([0.00026786]),\n",
       " (0, 5, 6, 7, 9, 12): array([0.00108333]),\n",
       " (2, 5, 7, 9, 10, 12): array([0.00095238]),\n",
       " (2, 4, 5, 7, 9, 10, 12): array([5.95238095e-05]),\n",
       " (0, 1, 5, 7, 10, 11, 12): array([-0.00011905]),\n",
       " (0, 4, 7, 9, 12): array([-0.00087302]),\n",
       " (0, 4, 6, 7, 12): array([1.88964475e-06]),\n",
       " (2, 4, 7, 11, 12): array([-0.00232143]),\n",
       " (0, 4, 6, 12): array([-0.00200582]),\n",
       " (0, 2, 4, 5, 6, 11, 12): array([-0.00090476]),\n",
       " (6, 7, 8, 12): array([0.00146825]),\n",
       " (0, 2, 4, 5, 6, 7, 11, 12): array([0.00030952]),\n",
       " (0, 11, 12): array([-0.01184168]),\n",
       " (0, 3, 5, 11, 12): array([0.01017857]),\n",
       " (0, 2, 4, 6, 10, 12): array([-0.00011905]),\n",
       " (0, 3, 11, 12): array([-0.00112374]),\n",
       " (0, 2, 4, 6, 12): array([8.92857143e-05]),\n",
       " (5, 6, 7, 8, 10, 11, 12): array([0.00198413]),\n",
       " (4, 5, 11, 12): array([0.00240079]),\n",
       " (0, 4, 5, 6, 7, 9, 11, 12): array([0.00037628]),\n",
       " (0, 6, 11, 12): array([0.00165816]),\n",
       " (0, 6, 12): array([0.00189935]),\n",
       " (0, 5, 6, 7, 8, 10, 11, 12): array([-0.00041667]),\n",
       " (0, 2, 4, 5, 7, 8, 9, 11, 12): array([0.00044643]),\n",
       " (6, 7, 9, 12): array([0.0006746]),\n",
       " (6, 7, 9, 11, 12): array([0.00011905]),\n",
       " (0, 4, 5, 6, 7, 12): array([0.00021429]),\n",
       " (1, 4, 5, 7, 11, 12): array([4.29550575e-18]),\n",
       " (2, 5, 6, 12): array([-0.00809524]),\n",
       " (5, 6, 7, 9): array([0.00723498]),\n",
       " (2, 5, 6, 7, 9, 10): array([-0.00264881]),\n",
       " (5, 6, 7, 9, 10): array([1.7182023e-17]),\n",
       " (1, 5, 9, 10, 11, 12): array([0.00083333]),\n",
       " (0, 1, 2, 5, 6, 7, 9, 10): array([-4.13029399e-18]),\n",
       " (4, 5, 6, 7, 10, 12): array([0.0002381]),\n",
       " (4, 5, 7, 11, 12): array([-0.0071875]),\n",
       " (0, 2, 5, 6, 7, 9, 10): array([0.00229167]),\n",
       " (5, 6, 7, 9, 10, 11, 12): array([-3.96825397e-05]),\n",
       " (0, 5, 6, 8, 11, 12): array([0.00142857]),\n",
       " (0, 5, 8, 12): array([0.00109821]),\n",
       " (0, 2, 5, 9, 10, 12): array([-0.00597884]),\n",
       " (0, 5, 8, 11, 12): array([0.00144345]),\n",
       " (6, 8, 11, 12): array([-0.00015873]),\n",
       " (6, 11, 12): array([0.00327492]),\n",
       " (6, 7, 8, 11, 12): array([3.96825397e-05]),\n",
       " (2, 5, 6, 7, 11, 12): array([-0.00050759]),\n",
       " (0, 2, 5, 7, 8, 9, 10, 11, 12): array([0.00077381]),\n",
       " (2, 4, 5, 6, 7, 11, 12): array([0.00384921]),\n",
       " (0, 2, 4, 6, 7, 10, 11, 12): array([-0.00039116]),\n",
       " (0, 2, 4, 7, 10, 11, 12): array([0.0005102]),\n",
       " (0, 2, 4, 10, 11, 12): array([-0.00036111]),\n",
       " (0, 2, 4, 12): array([-0.00776355]),\n",
       " (4, 6, 7, 11, 12): array([-7.93650794e-05]),\n",
       " (0, 2, 4, 11, 12): array([-0.0001873]),\n",
       " (0, 2, 4, 5, 7, 10, 11, 12): array([0.00047619]),\n",
       " (0, 1, 5, 9, 10, 12): array([-0.00178095]),\n",
       " (0, 2, 5, 6, 7, 10, 11, 12): array([0.0015873]),\n",
       " (0, 1, 5, 6, 9, 10, 12): array([0.00212302]),\n",
       " (0, 2, 5, 7, 8, 9, 12): array([0.00074735]),\n",
       " (0, 5, 6, 9, 10, 12): array([0.00623214]),\n",
       " (0, 1, 2, 5, 6, 9, 10, 12): array([0.00011905]),\n",
       " (0, 5, 7, 8, 9, 12): array([-0.00073413]),\n",
       " (0, 2, 5, 6, 7, 8, 10, 11, 12): array([0.0018254]),\n",
       " (0, 5, 6, 7, 9, 10, 11, 12): array([0.00025162]),\n",
       " (0, 4, 5, 7, 10, 12): array([0.00450893]),\n",
       " (0, 2, 5, 6, 7, 10, 12): array([-0.00308303]),\n",
       " (0, 5, 7, 8, 12): array([0.002]),\n",
       " (0, 2, 5, 7, 8, 10, 12): array([0.00011905]),\n",
       " (0, 5, 8, 10, 12): array([0.00190476]),\n",
       " (0, 2, 5, 7, 10, 12): array([0.00069048]),\n",
       " (0, 5, 7, 8, 10, 12): array([-0.00076099]),\n",
       " (1, 5, 6, 7, 8, 10, 11, 12): array([-5.95238095e-05]),\n",
       " (0, 5, 7, 8, 10, 11, 12): array([-0.0030119]),\n",
       " (1, 5, 6, 7, 8, 10, 12): array([-0.00052778]),\n",
       " (0, 5, 9, 11, 12): array([0.00045635]),\n",
       " (3, 4, 5, 6, 7, 11, 12): array([-0.00053571]),\n",
       " (5, 8, 9, 10, 11, 12): array([-0.00143141]),\n",
       " (0, 1, 5, 6, 9, 10, 11, 12): array([3.96825397e-05]),\n",
       " (0, 1, 5, 6, 8, 9, 10, 11, 12): array([0.00035714]),\n",
       " (5, 6, 9, 10, 12): array([0.00360714]),\n",
       " (0, 5, 6, 7, 9, 10): array([-0.00065476]),\n",
       " (2, 5, 9, 11, 12): array([-0.01464286]),\n",
       " (0, 7, 8, 12): array([0.00181548]),\n",
       " (2, 4, 5, 11, 12): array([-0.00087302]),\n",
       " (2, 4, 6, 7, 12): array([0.00089286]),\n",
       " (4, 5, 6, 7, 9, 10, 11, 12): array([-0.00034014]),\n",
       " (0, 4, 5, 6, 9, 11, 12): array([0.00031746]),\n",
       " (2, 4, 7, 12): array([0.01490079]),\n",
       " (0, 2, 4, 5, 7, 8, 9, 10, 12): array([0.0012619]),\n",
       " (0, 4, 5, 6, 7, 9, 10, 11, 12): array([0.00014456]),\n",
       " (1, 6, 7, 11, 12): array([0.00014881]),\n",
       " (0, 2, 4, 5, 8, 9, 10, 12): array([-0.00085714]),\n",
       " (0, 4, 5, 10, 11, 12): array([0.00031746]),\n",
       " (5, 6, 11, 12): array([-0.00277778]),\n",
       " (0, 3, 4, 12): array([0.00320437]),\n",
       " (0, 3, 4, 9, 12): array([-0.00031746]),\n",
       " (0, 6, 7, 8, 10, 11, 12): array([-8.5910115e-18]),\n",
       " (4, 5, 7, 8, 12): array([0.00021755]),\n",
       " (4, 5, 7, 8, 10, 12): array([1.48809524e-05]),\n",
       " (0, 4, 5, 7, 9, 11, 12): array([-0.00035714]),\n",
       " (5, 6, 9, 12): array([-0.00292092]),\n",
       " (5, 6, 7): array([0.00345238]),\n",
       " (4, 5, 7): array([0.00345238]),\n",
       " (2, 5, 7, 8, 10, 12): array([0.0010119]),\n",
       " (0, 5, 6, 7, 11, 12): array([0.00119841]),\n",
       " (2, 4, 5, 6, 7, 9, 10, 11, 12): array([7.93650794e-05]),\n",
       " (2, 4, 5, 6, 7, 9, 10, 12): array([0.00046685]),\n",
       " (0, 4, 5, 6, 10, 11, 12): array([-0.00107143]),\n",
       " (1, 5, 9, 12): array([0.04798942]),\n",
       " (4, 6, 10, 11, 12): array([-0.01031746]),\n",
       " (3, 5, 10, 11, 12): array([0.00313657]),\n",
       " (5, 8, 12): array([0.00689286]),\n",
       " (4, 5, 7, 9, 10, 11, 12): array([-9.52380952e-05]),\n",
       " (5, 7, 10): array([-0.00419841]),\n",
       " (0, 2, 5, 12): array([3.40136054e-05]),\n",
       " (5, 8, 10, 12): array([-0.00847222]),\n",
       " (6, 12): array([0.00153686]),\n",
       " (5, 7, 10, 11): array([0.00079365]),\n",
       " (4, 6, 11, 12): array([0.00210317]),\n",
       " (3, 5, 8, 10, 11, 12): array([0.00040179]),\n",
       " (1, 5, 7, 9, 10, 12): array([0.00083333]),\n",
       " (5, 7, 8, 9, 10, 11, 12): array([-2.97619048e-05]),\n",
       " (3, 4, 5, 7, 12): array([0.00059524]),\n",
       " (2, 5, 6, 11, 12): array([-0.00198413]),\n",
       " (5, 7, 8, 10, 11, 12): array([-0.00044643]),\n",
       " (0, 5, 7, 9): array([-0.00121429]),\n",
       " (0, 4, 5, 6, 11, 12): array([-0.00029762]),\n",
       " (0, 4, 5, 7, 8, 9, 10, 12): array([-0.00035714]),\n",
       " (2, 4, 5, 7, 9, 12): array([0.00020238]),\n",
       " (1, 5, 10, 12): array([0.00053571]),\n",
       " (2, 5, 8, 11, 12): array([0.00031746]),\n",
       " (4, 5, 8, 10, 12): array([0.00142857]),\n",
       " (1, 5, 9, 11, 12): array([0.00119048]),\n",
       " (9, 11, 12): array([-0.00094048]),\n",
       " (5, 11, 12): array([-0.00324993]),\n",
       " (2, 3, 4, 5, 6, 10, 11, 12): array([-3.96825397e-05]),\n",
       " (4, 5, 6, 10, 12): array([0.00031746]),\n",
       " (0, 2, 5, 6, 10, 12): array([0.00017857]),\n",
       " (2, 5, 9, 12): array([0.00160714]),\n",
       " (0, 2, 5, 6, 7, 8, 9, 12): array([0.00057823]),\n",
       " (0, 1, 4, 5, 6, 9, 10, 12): array([0.00060516]),\n",
       " (0, 1, 4, 5, 6, 7, 9, 10, 12): array([0.00020833]),\n",
       " (0, 2, 5, 6, 7, 8, 9, 10, 12): array([-0.00331633]),\n",
       " (2, 4, 5, 7, 8, 9, 10, 11, 12): array([0.00047619]),\n",
       " (2, 4, 5, 7, 8, 9, 10, 12): array([-0.00027211]),\n",
       " (2, 5, 9, 10, 12): array([-0.00134271]),\n",
       " (1, 5, 7, 12): array([-0.00507143]),\n",
       " (1, 2, 4, 5, 7, 9, 12): array([-0.00029762]),\n",
       " (2, 5, 7, 11, 12): array([0.0032483]),\n",
       " (1, 5, 7, 10, 12): array([0.00045635]),\n",
       " (0, 1, 5, 7, 9, 10, 12): array([0.00017857]),\n",
       " (2, 5, 6, 7, 9, 10, 11, 12): array([0.00057143]),\n",
       " (0, 2, 5, 7, 9, 12): array([0.00017857]),\n",
       " (1, 5, 7, 9, 11, 12): array([0.00040816]),\n",
       " (2, 4, 5, 6, 7, 9, 12): array([0.00118254]),\n",
       " (0, 9, 11, 12): array([-0.00083333]),\n",
       " (2, 4, 5, 6, 7, 8, 9, 12): array([-0.00015873]),\n",
       " (2, 6, 10, 12): array([-0.00267857]),\n",
       " (6, 10, 12): array([0.01152778]),\n",
       " (0, 6, 7, 8, 9, 10, 11, 12): array([-5.95238095e-05]),\n",
       " (2, 4, 5, 9, 12): array([-0.00066667]),\n",
       " (0, 2, 4, 5, 6, 7, 9, 10): array([0.000625]),\n",
       " (0, 2, 5, 8, 10, 11, 12): array([0.00043651]),\n",
       " (0, 5, 8, 10, 11, 12): array([0.00054563]),\n",
       " (0, 1, 2, 5, 10, 11, 12): array([0.00059524]),\n",
       " (0, 1, 5, 10, 11, 12): array([0.00279762]),\n",
       " (5, 6, 9, 11, 12): array([0.00563492]),\n",
       " (0, 4, 5, 6, 7, 8, 12): array([0.00014286]),\n",
       " (0, 2, 4, 5, 6, 7, 12): array([0.00103175]),\n",
       " (5, 6, 10, 11, 12): array([0.0022619]),\n",
       " (0, 1, 5, 7, 9, 10, 11, 12): array([-0.00011905]),\n",
       " (3, 4, 5, 10, 11, 12): array([0.00190476]),\n",
       " (4, 5, 10, 11, 12): array([0.00054422]),\n",
       " (0, 4, 8, 11, 12): array([-0.00027211]),\n",
       " (0, 2, 5, 6, 7, 11, 12): array([0.002]),\n",
       " (2, 5, 7, 9, 10, 11, 12): array([0.00034014]),\n",
       " (0, 2, 5, 7, 9, 10, 11, 12): array([0.00090136]),\n",
       " (0, 2, 5, 9, 10, 11, 12): array([-0.0024966]),\n",
       " (3, 4, 6, 7, 10, 11, 12): array([-5.95238095e-05]),\n",
       " (3, 6, 7, 10, 11, 12): array([8.92857143e-05]),\n",
       " (3, 6, 7, 8, 12): array([0.00130952]),\n",
       " (0, 3, 7, 11, 12): array([-0.00027778]),\n",
       " (4, 5, 6, 7, 8, 12): array([-0.00020833]),\n",
       " (0, 5, 6, 10, 12): array([0.00690476]),\n",
       " (0, 2, 4, 6, 10, 11, 12): array([0.00025794]),\n",
       " (1, 5, 7, 9, 10, 11, 12): array([-0.00088435])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_contrib1 = utils.aggregated_contribution(contributions1)\n",
    "mean_contrib2 = utils.aggregated_contribution(contributions2)\n",
    "mean_contrib1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 7** : Observez le contenu de la variable *mean_contrib1*. Quel est son type ? A quoi correspond son contenu ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"text-decoration:underline\">Réponse</span> : C'est un dictionnaire dont les clés représentent les indices des variables des données de Boston, et dont les valeurs représentent la contribution moyenne pour la/les variables correspondant(es)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 8** : Vérifiez que la différence entre les contributions des variables (et intéractions de variables) sommée est égale à la différence entre les prédictions moyennes entre les 2 sous échantillons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3143193277310927, 1.3143193277310914)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(list(mean_contrib1.values())) - np.sum(list(mean_contrib2.values())),np.mean(prediction1) - np.mean(prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 9** : En sachant que pour un indice de variable donné, il est possible d'en récupérer le nom via le code suivant :\n",
    "`indice_variable = 1\n",
    "boston[\"feature_names\"][indice_variable]`\n",
    "\n",
    "\n",
    "Calculer de nouveau les différences de contributions entre les 2 sous échantillons, en prenant cette fois en compte les intéractions entre variables.\n",
    "Afficher par exemple, les 10 variables/intéractions de variables qui ont le plus contribuées aux écarts de prix entre les 2 échantillons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LSTAT'] [0.28211923]\n",
      "['RM'] [0.26142737]\n",
      "['RM', 'DIS', 'LSTAT'] [0.24928681]\n",
      "['RM', 'LSTAT'] [0.21326153]\n",
      "['RM', 'PTRATIO', 'LSTAT'] [0.07639257]\n",
      "['CRIM', 'NOX', 'DIS', 'LSTAT'] [-0.06303489]\n",
      "['RM', 'PTRATIO', 'B', 'LSTAT'] [0.03671815]\n",
      "['ZN', 'RM', 'TAX', 'LSTAT'] [0.03656458]\n",
      "['AGE', 'LSTAT'] [0.03570942]\n",
      "['RM', 'DIS', 'RAD', 'LSTAT'] [0.03485015]\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for k in set(mean_contrib1.keys()).union(\n",
    "              set(mean_contrib2.keys())):\n",
    "    res.append(([boston[\"feature_names\"][index] for index in k] , \n",
    "               mean_contrib1.get(k, 0) - mean_contrib2.get(k, 0)))   \n",
    "         \n",
    "for lst, v in (sorted(res, key=lambda x:-abs(x[1])))[:10]:\n",
    "    print (lst, v)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 10** :  Représenter les *features importances* du modèle *rf*. Interprétez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2c5a02d5fd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "feats = {} \n",
    "for feature, importance in zip(X_train.columns, rf.feature_importances_):\n",
    "    feats[feature] = importance \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "importances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focus sur les Forêts Aléatoires en Classification\n",
    "\n",
    "Nous utiliserons ici le dataset **iris**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.DataFrame(iris.target, columns=[\"Classe\"])\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice Bilan** : Sur la base des données *iris*, appliquez une forêt aléatoire et utilisez le package **treeinterpreter** pour décomposer la prédiction pour certains points de données choisit manuellement.\n",
    "\n",
    "En classification, les fonctions vues précédemment s'utilisent de la même manière mais les sorties sont différentes.. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction [[0. 1. 0.]]\n",
      "Priormean [[0.348 0.314 0.338]]\n",
      "Feature contributions:\n",
      "sepal length (cm) [ 0.          0.00353159 -0.00353159]\n",
      "sepal width (cm) [ 0.        -0.0184375  0.0184375]\n",
      "petal length (cm) [-0.12525     0.32571547 -0.20046547]\n",
      "petal width (cm) [-0.22275     0.37519045 -0.15244045]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=1234)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=1/3, random_state=1234)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "onerow = X_test.iloc[:1,]\n",
    "\n",
    "prediction, priormean, contributions = ti.predict(rf, onerow)\n",
    "print(\"Prediction\", prediction)\n",
    "print(\"Priormean\", priormean)\n",
    "print(\"Feature contributions:\")\n",
    "for c, feature in zip(contributions[0],iris.feature_names):\n",
    "    print(feature, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Décomposer et rendre interprétable les prévisions des forêts aléatoires est finalement assez simple. Cela peut conduire à un niveau d'interprétabilité proche à celui des modèles linéaires. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
