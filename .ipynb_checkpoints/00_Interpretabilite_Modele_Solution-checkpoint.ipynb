{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interprétabilité des modèles\n",
    "## 0. Overview\n",
    "\n",
    "Les enjeux derrière l'interprétabilité des modèles, ses motivations ainsi que des méthodes pour les appliquer ont été présentées par **Maxence Brochard**. Vous pouvez à tout moment retrouver les ressources ici : https://lion.app.box.com/folder/65170147483 \n",
    "\n",
    "=> **L'objectif de ce notebook est la mise en pratique de l'interprétabilité des modèles sur des cas concrets**\n",
    "\n",
    "### 0.1 Données\n",
    "\n",
    "Pour la suite de ce notebook, nous nous baserons sur un jeu de données très simple, qui vise à déterminer le prix de vente des maisons de Boston en fonction de différents indicateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target, columns=[\"Houses prices\"])\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Focus sur les Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer, vous devez installer le package **treeinterpreter** : `pip install treeinterpreter` dans votre CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings # On enlève les warnings pour la suite du notebook\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treeinterpreter import treeinterpreter as ti\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Idée générale\n",
    "\n",
    "Nous allons essayer de nous intéresser un peu plus en profondeur aux Random Forests, et plus particulièrement aux prédictions obtenus par ces modèles, en cherchant à décomposer une prédiction, par exemple pour une maison *i* comme la somme des contributions de chaque variable pour cette maison, i.e. : \n",
    "<center> $ prediction^i=biais+contributionVariable_1^i+…+contributionVariable_n^i $ </center> \n",
    "\n",
    "Très peu de packages proposent actuellement de rentrer dans ce niveau de détails à l'heure actuelle. Nous nous baserons sur **treeinterpreter**. Ce dernier propose la décomposition exposée au dessus pour différents modèles existants sous *scikit-learn*, tels que :\n",
    "* DecisionTreeRegressor\n",
    "* DecisionTreeClassifier\n",
    "* ExtraTreeRegressor\n",
    "* ExtraTreeClassifier\n",
    "* RandomForestRegressor\n",
    "* RandomForestClassifier\n",
    "* ExtraTreesRegressor\n",
    "* ExtraTreesClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Première Random Forest\n",
    "**Exercice :**\n",
    "Pour commencer, séparez les données à disposition en 2 échantillons (via la fonction *train_test_split*) en définissant une graîne aléatoire (=1234) :\n",
    "* un échantillon d'apprentissage (2/3 des données)\n",
    "* un échantillon de validation (1/3 des données)\n",
    "\n",
    "Entraînez ensuite une Random Forest (les paramètres par défaut suffiront pour l'exemple) sur la base de vos données d'apprentissage, en indiquant une graine aléatoire (=1234) pour figer les résultats.\n",
    "\n",
    "*NB : Nous noterons X_train, X_test, y_train, y_test nos échantillons d'apprentissages et de validation, et rf le modèle entraîné*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=1234, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=1/3, random_state=1234)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=1234)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant choisir 2 points de données de notre échantillon de test, sur lequel nous allons prédire notre cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction pour ligne 0 de notre échantillon de test : 32.18\n",
      "Prédiction pour ligne 1 de notre échantillon de test : 23.78\n"
     ]
    }
   ],
   "source": [
    "tworows = X_test.iloc[:2,]\n",
    "for i,prediction in enumerate(rf.predict(tworows)):\n",
    "    print(\"Prédiction pour ligne {} de notre échantillon de test : {}\".format(i,round(prediction,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que les prédictions sont très éloignées pour ces 2 points de données. L'idée est donc de comprendre maintenant quelles sont les variables qui ont le plus contribuées (aussi bien négativement que positivement) les prédictions.\n",
    "\n",
    "Nous allons donc utiliser le package **treeinterpreter**.\n",
    "\n",
    "La structure est relativement simple, et nous permet, sur la base d'un modèle déjà entraîné, de récupérer pour des points de données de l'échantillon de test, la prédiction du modèle, le biais, ainsi que les contributions de chaque variable :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, biais, contributions = ti.predict(rf, tworows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'on commence par s'intéresser au contenu de *prediction*, on remarque bien qu'on récupère les mêmes valeurs que ci-dessus. (à ceci près qu'elles sont arrondies à deux décimales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32.18],\n",
       "       [23.78]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le contenu de la variable *biais*, qui n'est rien de plus que la moyenne sur l'échantillon d'apprentissage, est logiquement le même qu'importe le point de données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.424273, 22.424273])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, la variable *contributions* contient deux arrays de dimensions 1x13 chacun, représentant pour chaque prédiction, la contribution de chacune des variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.60555556e-01,  0.00000000e+00,  7.50000000e-02,\n",
       "         0.00000000e+00,  2.46911765e-01,  6.68611695e+00,\n",
       "         4.71296296e-03, -9.32726952e-01,  4.50000000e-02,\n",
       "         1.76981430e+00,  1.61194290e-01, -2.28973138e-01,\n",
       "         1.26812127e+00],\n",
       "       [-2.89914054e-01, -8.12566845e-02, -7.07142857e-02,\n",
       "         0.00000000e+00,  1.82911765e-01,  1.84975041e-01,\n",
       "        -2.87870370e-02,  7.94091021e-02,  2.83180189e-01,\n",
       "        -3.20409387e-02, -1.88606092e+00, -6.34404762e-02,\n",
       "         3.07746530e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc tout ce qui est nécessaire pour déterminer les contributions de chacune des variables aux deux prédictions, en rappelant que :\n",
    "<center> $ prediction=biais+contributionVariable1+…+contributionVariablen $ </center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point de données 0\n",
      "Biais 22.424272997032645\n",
      "Contributions des variables (par décroissante absolue) :\n",
      "RM : 6.69\n",
      "TAX : 1.77\n",
      "LSTAT : 1.27\n",
      "DIS : -0.93\n",
      "CRIM : 0.66\n",
      "NOX : 0.25\n",
      "B : -0.23\n",
      "PTRATIO : 0.16\n",
      "INDUS : 0.07\n",
      "RAD : 0.05\n",
      "AGE : 0.0\n",
      "ZN : 0.0\n",
      "CHAS : 0.0\n",
      "--------------------\n",
      "Point de données 1\n",
      "Biais 22.424272997032645\n",
      "Contributions des variables (par décroissante absolue) :\n",
      "LSTAT : 3.08\n",
      "PTRATIO : -1.89\n",
      "CRIM : -0.29\n",
      "RAD : 0.28\n",
      "RM : 0.18\n",
      "NOX : 0.18\n",
      "ZN : -0.08\n",
      "DIS : 0.08\n",
      "INDUS : -0.07\n",
      "B : -0.06\n",
      "TAX : -0.03\n",
      "AGE : -0.03\n",
      "CHAS : 0.0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tworows)):\n",
    "    print(\"Point de données {}\".format(i))\n",
    "    print(\"Biais {}\".format(biais[i]))\n",
    "    print(\"Contributions des variables (par décroissante absolue) :\")\n",
    "    for c, feature in sorted(zip(contributions[i], \n",
    "                                 boston.feature_names), \n",
    "                             key=lambda x: -abs(x[0])):\n",
    "        print(\"{} : {}\".format(feature, round(c, 2)))\n",
    "    print(\"-\"*20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : A partir des informations récupérées (biais et contributions) grâce au module *.predict* du package **treeinterpreter**, recalculer les prédictions pour nos 2 points de données de test, et vérifier qu'elles correspondent bien à ce que nous avions obtenu avec *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[32.18],\n",
       "        [23.78]]), array([32.18, 23.78]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction,biais + np.sum(contributions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce package est donc très pratique pour pouvoir mieux interpréter les prédictions de nos modèles de Random Forests pour certains points de données.\n",
    "\n",
    "On y trouve **2 intérêts majeurs** :\n",
    "* Comprendre pourquoi les valeurs prédites sur 2 jeux de données sont différentes, et quelles sont les variables en causes. Sur notre exemple, on pourrait par exemple chercher à comprendre d'où viennent les différences de prix des maisons de plusieurs voisinages\n",
    "* Débugger un modèle et/ou les données, en cherchant par exemple à comprendre pourquoi les valeurs prédites sur un nouveau jeu de données ne matchent pas avec celle d'anciennes données\n",
    "\n",
    "=> **Essayons de développer le premier cas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Splitter le jeu de données de test *(X_test)* en 2 sous échantillons (respectivement *ech1* & *ech2*) de tailles égales. En utilisant la Random Forest déjà entraînée, calculez et stockez les prédictions associées à ces 2 sous échantillons. Calculez en ensuite la moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.977142857142855 21.662823529411764\n"
     ]
    }
   ],
   "source": [
    "idx = round(len(X_test)/2)\n",
    "ech1 = X_test.iloc[:idx,:]\n",
    "ech2 = X_test.iloc[idx:,:]\n",
    "\n",
    "pred_ech1 = rf.predict(ech1)\n",
    "pred_ech2 = rf.predict(ech2)\n",
    "print(np.mean(pred_ech1),np.mean(pred_ech2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut constater que les prédictions moyennes sont relativement différentes sur les 2 échantillons.\n",
    "\n",
    "**Exerice** : Appliquer la décomposition vue au dessus sur les 2 sous échantillons *ech1* et *ech2*. On notera *prediction1, biais1* et *contributions1* (respectivement *2*) les variables dans lequelles seront stockées les résultats.\n",
    "\n",
    "Sommez ensuite les contributions par variable pour chaque sous échantillon dans deux variables, respectivement *totalc1* et *totalc2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1, biais1, contributions1 = ti.predict(rf, ech1)\n",
    "prediction2, biais2, contributions2 = ti.predict(rf, ech2)\n",
    "\n",
    "totalc1 = np.mean(contributions1, axis=0) \n",
    "totalc2 = np.mean(contributions2, axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la mesure où les biais sont les mêmes (puisque calculés sur le même échantillon d'apprentissage), la différence entre les prédictions moyennes sur les 2 sous échantillons provient uniquement des contributions des différentes variables.\n",
    "Dans d'autres termes, la somme des contributions des variables est égale à la différence entre les prédictions moyennes. Ce que nous pouvons facilement vérifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3143193277310923, 1.3143193277310914)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(totalc1 - totalc2),np.mean(prediction1) - np.mean(prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** : Calculer les différences de contributions de chaque variable entre les 2 sous échantillons, et appuyez vous sur le dictionnaire des données disponible en début de notebook pour interpréter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTAT : 0.45\n",
      "RM : 0.43\n",
      "DIS : 0.19\n",
      "B : 0.13\n",
      "AGE : 0.11\n",
      "PTRATIO : 0.08\n",
      "RAD : 0.05\n",
      "TAX : 0.03\n",
      "ZN : 0.03\n",
      "CHAS : 0.0\n",
      "INDUS : -0.02\n",
      "CRIM : -0.07\n",
      "NOX : -0.1\n"
     ]
    }
   ],
   "source": [
    "for c, variable in sorted(zip(totalc1 - totalc2, \n",
    "                             boston.feature_names), reverse=True):\n",
    "    print('{} : {}'.format(variable, round(c, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crédits\n",
    "Nous allons baser la suite sur le lien suivant : http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
